name: DSE Market Depth Scraper

on:
  schedule:
    # Run at 8:00 UTC (2:00 PM Bangladesh time)
    - cron: '0 8 * * 1-5'
    # Run every 10 minutes from 8:10 to 10:50 UTC (2:10 PM to 4:50 PM Bangladesh time) 0 = Sunday
      # 1 = Monday
      # 2 = Tuesday
      # 3 = Wednesday
      # 4 = Thursday
      # 5 = Friday
      # 6 = Saturday
    # - cron: '10,20,30,40,50 8-10 * * 1-5'
    # Run at 10, 20, and 30 minutes past 8 AM UTC (2:10, 2:20, and 2:30 PM Bangladesh time) for Sunday to Thursday, you would use 0-4
    - cron: '10,20,30 8 * * 0-4'
  workflow_dispatch:  # Allow manual triggering
  
permissions: # Top level key, placed correctly
  contents: read
  packages: write
  pull-requests: write
  
jobs:
  scrape:
    # runs-on: ubuntu-latest
    runs-on: ubuntu-20.04
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Set up Chrome and ChromeDriver
      run: |
        # Install Chrome if needed
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable  
        
    - name: Clear webdriver-manager cache
      run: |
        echo "Clearing webdriver-manager cache..."
        rm -rf /home/runner/.wdm/
        echo "Cache cleared."
        
    - name: Install Chrome and dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y chromium-browser
        sudo apt-get install -y libgconf-2-4
        sudo apt-get install -y wget unzip xvfb libxi6 libgconf-2-4
        wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
        sudo apt install -y ./google-chrome-stable_current_amd64.deb
        sudo apt-get install -y google-chrome-stable
      
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create token file
      run: |
        echo "${{ secrets.CREDENTIALS_JSON }}" > credentials.json

# #  Python script to handle the authentication and generate token.pickle.
#     - name: Generate token.pickle
#       run: python generate_token.py
      
# #You will use the drive folder id in later python scripts, that interact with the google drive api.
#     - name: Your drive actions
#       run: python drive_actions.py
        
          #Look for a section labeled "Artifacts" at the bottom of the page
    - name: Save credential files as artifacts
      # uses: actions/upload-artifact@v2
      uses: actions/upload-artifact@v4  # Change from v2 to v4
      with:
        name: google-credentials
        path: |
          credentials.json
          token.pickle    
          
    - name: Run scraper
      env:
        # DRIVE_FOLDER_ID: ${{ secrets.DRIVE_FOLDER_ID }}
        # CREDENTIALS_JSON: ${{ secrets.CREDENTIALS_JSON }}
        SERVICE_ACCOUNT_KEY: ${{ secrets.SERVICE_ACCOUNT_KEY }}
        DRIVE_FOLDER_ID: ${{ secrets.DRIVE_FOLDER_ID }}
      run: python main.py
      
      # ADD DEBUG STEP HERE
    - name: List files after save
      run: |
        echo "=== Current directory structure ==="
        ls -l
        echo "=== Excel file check ==="
        find . -name "Market_1_Depth_*.xlsx"


