name: DSE Market Depth Scraper

# on:
#   schedule:
#       # Run at 10, 20, and 30 minutes past 8 AM UTC (2:10, 2:20, and 2:30 PM Bangladesh time) for Sunday to Thursday, you would use 0-4
#    - cron: '10,20,30 8 * * 0-4'
#     # Run at 8:00 UTC (2:00 PM Bangladesh time)
#     # - cron: '0 8 * * 0-4'
#     # Run every 10 minutes from 8:10 to 10:50 UTC (2:10 PM to 4:50 PM Bangladesh time) 0 = Sunday
#       # 1 = Monday
#       # 2 = Tuesday
#       # 3 = Wednesday
#       # 4 = Thursday
#       # 5 = Friday
#       # 6 = Saturday
#   workflow_dispatch:  # Allow manual triggering

on:
  schedule:
    # Run at 0, 10, 20, and 27 minutes past 8 AM UTC (2:00, 2:10, 2:20, and 2:30 PM Bangladesh time) for Sunday to Thursday
    # - cron: '0,10,20,27 8 * * 0-4'
    - cron: '30,40 7 * * 0-4'
  workflow_dispatch:  # Allow manual triggering
  
permissions: # Top level key, placed correctly
  contents: read
  packages: write
  pull-requests: write
  
jobs:
  scrape:
    runs-on: ubuntu-latest
    # runs-on: ubuntu-20.04
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Set up Chrome and ChromeDriver
      run: |
        # Install Chrome if needed
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable  
        
    - name: Clear webdriver-manager cache
      run: |
        echo "Clearing webdriver-manager cache..."
        rm -rf /home/runner/.wdm/
        echo "Cache cleared."
        
    # - name: Install Chrome and dependencies
    #   run: |
    #     sudo apt-get update
    #     sudo apt-get install -y chromium-browser
    #     sudo apt update
    #     sudo apt-get install -y libgconf-2-4
    #     sudo apt-get install -y wget unzip xvfb libxi6 libgconf-2-4
    #     wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
    #     sudo apt install -y ./google-chrome-stable_current_amd64.deb
    #     sudo apt-get install -y google-chrome-stable


    - name: Install Chrome and dependencies
      run: |
        # Update package lists
        sudo apt-get update
        
        # Add universe repository
        sudo add-apt-repository universe -y
        sudo apt-get update
        
        # Install alternative GConf package first
        sudo apt-get install -y libgconf2-4
        
        # Install other dependencies
        sudo apt-get install -y wget unzip xvfb libxi6
        
        # Download and install Chrome directly
        wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
        sudo apt-get install -y ./google-chrome-stable_current_amd64.deb

        
    # - name: Install Browser Dependencies
    #   run: |
    #     sudo apt-get update
    #     # Install required dependencies
    #     sudo apt-get install -y \
    #         libnss3 \
    #         libnspr4 \
    #         libatk1.0-0 \
    #         libatk-bridge2.0-0 \
    #         libcups2 \
    #         libdrm2 \
    #         libxkbcommon0 \
    #         libxcomposite1 \
    #         libxdamage1 \
    #         libxfixes3 \
    #         libxrandr2 \
    #         libgbm1 \
    #         libasound2
    #     # Install Chromium via snap (works on Ubuntu 24.04)
    #     sudo snap install chromium --classic


        
      
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create token file
      run: |
        echo "${{ secrets.CREDENTIALS_JSON }}" > credentials.json

        
          #Look for a section labeled "Artifacts" at the bottom of the page
    - name: Save credential files as artifacts
      uses: actions/upload-artifact@v4  # Change from v2 to v4
      with:
        name: google-credentials
        path: |
          credentials.json
          token.pickle    
          
    - name: Run scraper
      env:
        # DRIVE_FOLDER_ID: ${{ secrets.DRIVE_FOLDER_ID }}
        # CREDENTIALS_JSON: ${{ secrets.CREDENTIALS_JSON }}
        SERVICE_ACCOUNT_KEY: ${{ secrets.SERVICE_ACCOUNT_KEY }}
        DRIVE_FOLDER_ID: ${{ secrets.DRIVE_FOLDER_ID }}
      run: |
        echo "Run scraper"
        echo "$(date -u) UTC"
        echo "$(date -u --date='6 hours') Bangladesh Time"
        python main.py
      
      # ADD DEBUG STEP HERE
    - name: List files after save
      run: |
        echo "=== Current directory structure ==="
        ls -l
        echo "=== Excel file check ==="
        find . -name "Market_Depth_Auto_*.xlsx"


